# Multilingual Pre-trained Models Tutorial
This is a tutorial of multilingual pre-trained models maintained by the Beijing Jiaotong Natural Language Processing Group (Continual Updated).

The past five years have witnessed the rapid development of multilingual pre-trained models, especially for data-driven large language models (LLMs). Due to the dominance of multilingual NLP at the present time, priority is given to collecting important, up-to-date multilingual pre-trained models papers and their performance. As one of the contributions of "our paper", we continuously update and expand the content according to the chapters in "our paper". Our list is still incomplete and the categorization might be inappropriate. We will keep adding papers and improving the list. Any suggestions are welcome!
